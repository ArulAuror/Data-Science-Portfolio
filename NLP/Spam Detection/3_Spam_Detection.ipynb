{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"3_Spam_Detection.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["!pip install feature-engine"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lVzmmRxRT5-r","executionInfo":{"status":"ok","timestamp":1644507962550,"user_tz":360,"elapsed":12061,"user":{"displayName":"Shaannoor Mann","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02520257695567980696"}},"outputId":"a89c7802-9c1c-4406-e732-88dea3191cb1"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting feature-engine\n","  Downloading feature_engine-1.2.0-py2.py3-none-any.whl (205 kB)\n","\u001b[?25l\r\u001b[K     |█▋                              | 10 kB 15.1 MB/s eta 0:00:01\r\u001b[K     |███▏                            | 20 kB 10.9 MB/s eta 0:00:01\r\u001b[K     |████▉                           | 30 kB 8.4 MB/s eta 0:00:01\r\u001b[K     |██████▍                         | 40 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |████████                        | 51 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |█████████▋                      | 61 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 71 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 81 kB 5.8 MB/s eta 0:00:01\r\u001b[K     |██████████████▍                 | 92 kB 5.9 MB/s eta 0:00:01\r\u001b[K     |████████████████                | 102 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 112 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████▏            | 122 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████▊           | 133 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 143 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 153 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 163 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▏    | 174 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▊   | 184 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 194 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 204 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 205 kB 5.1 MB/s \n","\u001b[?25hRequirement already satisfied: pandas>=1.0.3 in /usr/local/lib/python3.7/dist-packages (from feature-engine) (1.3.5)\n","Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.7/dist-packages (from feature-engine) (1.4.1)\n","Requirement already satisfied: scikit-learn>=0.22.2 in /usr/local/lib/python3.7/dist-packages (from feature-engine) (1.0.2)\n","Requirement already satisfied: numpy>=1.18.2 in /usr/local/lib/python3.7/dist-packages (from feature-engine) (1.19.5)\n","Collecting statsmodels>=0.11.1\n","  Downloading statsmodels-0.13.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (9.8 MB)\n","\u001b[K     |████████████████████████████████| 9.8 MB 25.1 MB/s \n","\u001b[?25hRequirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.0.3->feature-engine) (2.8.2)\n","Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.0.3->feature-engine) (2018.9)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas>=1.0.3->feature-engine) (1.15.0)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.22.2->feature-engine) (1.1.0)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.22.2->feature-engine) (3.1.0)\n","Requirement already satisfied: patsy>=0.5.2 in /usr/local/lib/python3.7/dist-packages (from statsmodels>=0.11.1->feature-engine) (0.5.2)\n","Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.7/dist-packages (from statsmodels>=0.11.1->feature-engine) (21.3)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=21.3->statsmodels>=0.11.1->feature-engine) (3.0.7)\n","Installing collected packages: statsmodels, feature-engine\n","  Attempting uninstall: statsmodels\n","    Found existing installation: statsmodels 0.10.2\n","    Uninstalling statsmodels-0.10.2:\n","      Successfully uninstalled statsmodels-0.10.2\n","Successfully installed feature-engine-1.2.0 statsmodels-0.13.2\n"]}]},{"cell_type":"markdown","metadata":{"id":"_mJQGbTFOCAx"},"source":["<h1 align='center'><u>Spam Detection HW</u></h1>\n","\n","<font color = 'red' size = 4 >**Read complete instructions before starting the HW** </font>\n"]},{"cell_type":"markdown","metadata":{"id":"cXeG6ZQ4OVDj"},"source":["# Q1: Load the dataset  (1 Point)\n","\n","- For this Hw you will usespam dataset from kaggle which can be found from [this](https://www.kaggle.com/uciml/sms-spam-collection-dataset) link. You can download this data and either upload it in google drive or in colab workspace. Load the data in pandas dataframe. \n","\n","- There are only two useful columns. These columns are related to (1) label (ham and spam) and the (2) text of email.\n","\n","- Rename columns as label and message\n","\n","- Find the %  ham amd spam in the data.\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"pNdWCViagO4F"},"source":["# Q2 : Provide the metric for evaluating model (1 Point)\n","As you will notice, the  data is highly imbalanced (most messages are labelled as ham and only few are labelled as spam). Always predicting ham will give us very good accuracy (close to 90%). So you need to choose a different metric.\n","\n","Task: Provde the metric you will choose to evaluate your model. Explain why this is an appropriate metric for this case."]},{"cell_type":"markdown","metadata":{"id":"rH8_mvGhiThV"},"source":["# Q3 : Classification Pipelines (18 Points)\n","\n","In the previous lectures you learned Data processing, Featurization such as CountVectorizer, TFIDFVectorizer, and also Feature Engineering.\n","* You will now use folllowing methods to create fearures which you can use in your model. \n","\n","    1. Sparse Embeddings (TF-IDF) (6 Points)\n","    2. Feature Engineering (see examples below) (6 Points)\n","    3. Sparse Embeddings (TF-IDF) + Feature Engineering (6 Points)\n","\n","**Approach:**\n","\n","**Use a smaller subset of dataset (recommended 40 %) to evaluate the three pipelines . Based on your analysis (e.g. model score, learning curves) , choose one pipeline from the three. Provde your rational for choosing the pipleine. Train only the final pipeline on complete data.**\n","\n","**Requirements:** \n","\n","1. You will use XgBoost model for the classification. You will need to tune the **XGBoost for imbalanced dataset** (If you have never used XGBoost before , here is the link on XGBoost tutorial for imbalanced data: https://machinelearningmastery.com/xgboost-for-imbalanced-classification/).\n","\n","2. For feature engineering, you can choose from the examples below. You do not  have to use all of them. You can add other featues as well. Think about what faetures can distinguish a spam from a regular email. Some examples :\n","\n",">> Count of following  (Words, characters, digits, exclamation marks, numbers, Nouns, ProperNouns, AUX, VERBS, Adjectives, named entities, spelling mistakes (see the link on how to get spelling mistakes https://pypi.org/project/pyspellchecker/). \n","\n","3. For Sparse embeddings you will use **tfidf vectorization**. You need to choose appopriate parameters e.g. min_df, max_df, max_faetures, n-grams etc.). \n","\n","4. Think carefully about teh pre-processing you will do.\n","\n","Tip: <font color = 'red'>**Using GridSearch for hyperparameter tuning might take a lot of time. Try using RandomizedSearch.**</font> You can also explore faster implementation of Gridsearch and RandomizedSearch in sklearn: \n","\n","1. [Halving Grid Search](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.HalvingGridSearchCV.html?highlight=halving#sklearn.model_selection.HalvingGridSearchCV)\n","\n","2. [HalvingRandomSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.HalvingRandomSearchCV.html?highlight=halving#sklearn.model_selection.HalvingRandomSearchCV)\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"PqPTPb0AsMP3"},"source":["# **Required Submissions:**\n","1.  Submit two colab/jupyter notebooks\n","- (analysis with smaller subset and all three pipelines)\n","- (analysis with complete daya and only final pipeline)\n","2. Pdf version of the notebooks (HWs will not be graded if pdf version is not provided.\n","3. **The notebooks and pdf files should have the output.**\n","4. **Name files as follows : FirstName_file1_hw2, FirstName_file2_h2**"]}]}